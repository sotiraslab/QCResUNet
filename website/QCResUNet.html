
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="QCResUNet: Joint Subject-level and Voxel-level Prediction of Segmentation Quality">
    <meta name="author" content="Peijie Qiu,
                                Satrajit Chakrabarty,
                                Phuc Nguyen,
                                Soumyendu Sekhar Ghosh,
								Aristeidis Sotiras">

    <title>QCResUNet: Joint Subject-level and Voxel-level Prediction of Segmentation Quality</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
    <script type="text/javascript"
        src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js">
    </script>
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>QCResUNet: Joint Subject-level and Voxel-level <br> Prediction  of Segmentation Quality</h2>
    <h3>MICCAI 2023</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p class="authors">
        <a href=""> Peijie Qiu<sup>1</sup></a>,
        <a href=""> Satrajit Chakrabarty<sup>2</sup></a>,
        <a href=""> Phuc Nguyen<sup>3</sup></a>,</br>
        <a href=""> Soumyendu Sekhar Ghosh<sup>2</sup></a>,
        <a href=""> Aristeidis Sotiras<sup>1,4</sup></a>
    </p>
    <p>
        <a><sup>1</sup>Mallinckrodt Institute of Radiology, Washington University School of Medicine, St. Louis, MO, USA</a></br>
        <a><sup>2</sup>Department of Electrical and Systems Engineering, Washington University in St. Louis, St. Louis, MO, USA </a></br>
        <a><sup>3</sup>Department of Biomedical Engineering, University of Cincinnati, Cincinnati, OH, USA </a></br>
        <a><sup>4</sup>Institute for Informatics, Data Science & Biostatistics, Washington University School of Medicine, St. Louis, MO, USA </a></br>
    </p>

    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://link.springer.com/chapter/10.1007/978-3-031-43901-8_17">Paper [MICCAI 2023]</a>
        <a class="btn btn-primary" href="https://github.com/sotiraslab/QCResUNet">Code</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            Deep learning has achieved state-of-the-art performance in automated brain tumor segmentation from magnetic resonance imaging (MRI) scans. 
            However, the unexpected occurrence of poor-quality outliers, especially in out-of-distribution samples, hinders their translation into patient-centered clinical practice. 
            Therefore, it is important to develop automated tools for large-scale segmentation quality control (QC). 
            However, most existing QC methods targeted cardiac MRI segmentation which involves a single modality and a single tissue type. 
            Importantly, these methods only provide a subject-level segmentation-quality prediction, which cannot inform clinicians where the segmentation needs to be refined. 
            To address this gap, we proposed a novel network architecture called QCResUNet that simultaneously produces segmentation-quality measures as well as voxel-level segmentation error maps for brain tumor segmentation QC. 
            To train the proposed model, we created a wide variety of segmentation-quality results by using i) models that have been trained for a varying number of epochs with different modalities; and ii) a newly devised segmentation-generation method called SegGen. 
            The proposed method was validated on a large public brain tumor dataset with segmentations generated by different methods, achieving high performance on the prediction of segmentation-quality metric as well as voxel-wise localization of segmentation errors.
        </p>        
    </div>

    <div class="section">
        <h2>Joint Subject-level and Voxel-level Quality Prediction </h2>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="images/vis.jpg" style="width:100%">
                    <p style="text-align: justify;"><b>Figure 1:</b> Examples showcasing the performance of the proposed methods. The last column denotes the QC performance of different methods. The penultimate column denotes the predicted segmentation error.</p>
                </div>
            </div>

    <div class="section">
        <h2>QCResUNet</h2>
        <hr>
        <p>
            Given four imaging modalities denoted as $[X_1, X_2, X_3, X_4]$ and a predicted multi-class brain tumor segmentation mask ($S_{pred}$), 
            the goal of our approach is to automatically assess the tumor segmentation quality by simultaneously predicting DSC and identifying segmentation errors as a binary mask ($S_{err}$).
             Toward this end, we proposed a 3D encoder-decoder architecture termed QCResUNet (see Figure 1.(a)) for simultaneously predicting DSC and localizing segmentation errors. 
             QCResUNet has two parts trained in an end-to-end fashion: i) a ResNet-34 encoder for DSC prediction; and ii) a decoder architecture for segmentation error map prediction 
             (i.e., the difference between predicted segmentation and ground-truth segmentation). 
        </p>
        <img src="images/network.jpg" style="width:100%">
        <p style="text-align: justify;"><b>Figure 1:</b> (a) The proposed QCResUNet model adopts an encoder-decoder neural network architecture that takes four modalities and the segmentation to be evaluated. Given this input, QCResUNet predicts the DSC and segmentation error map. (b) The residual block in the encoder. (c) The convolutional block in the decoder.</p>
        <hr>
    </div>
    
    <div class="section">
        <h2>Multi-Task Learning</h2>
        <hr>
        <p>
            The training QCResUNet is under the multi-task learning framework, whose objective function consists of two parts. The first part corresponds to the DSC regression task. It consists of a mean absolute error (MAE) loss ($\mathcal{L}_{MAE}$) term that penalizes differences between ground truth ($DSC_{gt}$) and  predicted DSC ($DSC_{pred}$):
            <blockquote>
                <center>
                $\begin{eqnarray}
                \mathcal{L}_{MAE} = \frac{1}{N} \sum_{n=1}^{N} |DSC_{gt}^{(n)} - DSC_{pred}^{(n)}|_1,
                \end{eqnarray}$
                </center>
            </blockquote>
            where $N$ denotes the number of samples in a batch. 
            The second part of the objective function corresponds to the segmentation error prediction. 
            It consists of a dice loss~\cite{drozdzal2016importance} and a binary cross-entropy loss, given by:
            <blockquote>
                <center>
                $\begin{eqnarray}
                \mathcal{L}_{dice} = - \frac{2 \cdot \sum_{i=1}^{I} S_{err_{gt}}^{(i)} \cdot  S_{err_{pred}}^{(i)} }{\sum_{i=1}^{I}S_{err_{gt}}^{(i)} + \sum_{i=1}^{I}S_{err_{pred}}^{(i)}}
                <!-- \mathcal{L}_{BCE} = -\frac{1}{I} \sum_{i=1}^{I} S_{err_{gt}}^{(i)} \log S_{err_{pred}}^{(i)} + (1- S_{err_{gt}}^{(i)})\log(1-S_{err_{pred}}^{(i)}), -->
                <!-- $\end{eqnarray} -->
                </center>
            </blockquote>
            <!-- <hr> -->
            <blockquote>
                <center>
                $\begin{eqnarray}
                \mathcal{L}_{BCE} = -\frac{1}{I} \sum_{i=1}^{I} S_{err_{gt}}^{(i)} \log S_{err_{pred}}^{(i)} + (1- S_{err_{gt}}^{(i)}) \log \left( 1-S_{err_{pred}}^{(i)} \right)
                <!-- $\end{eqnarray} -->
                </center>
            </blockquote>
            where $S_{err_{gt}}$, $S_{err_{pred}}$ denote the binary ground-truth segmentation error map and the predicted error segmentation map from the sigmoid output of the decoder, respectively. The dice loss and cross-entropy loss were averaged across the number of pixels $I$ in a batch.
            The two parts are combined using a weight parameter $\lambda$ to balance the different loss components:
            <blockquote>
                <center>
                $\begin{eqnarray}
                \mathcal{L}_{total} = \mathcal{L}_{MAE} + \lambda (\mathcal{L}_{dice} + \mathcal{L}_{BCE}).
                <!-- $\end{eqnarray} -->
                </center>
            </blockquote>
            <hr>
        </p>
    </div>
    
    <div class="section">
        <h2>Validation on BraTS 2021 dataset</h2>
        <hr>
        <p>
           
        </p>
        <!-- <h2>Comparison to Baselines </h2> -->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <p style="text-align: justify;"><b>Table 1:</b> The QC performance of three baseline methods and the proposed method was evaluated on in-sample (nnUNet and SegGen) and out-of-sample (DeepMedic) segmentations. The best metrics in each column are highlighted in bold.</p>
                <img src="images/table.png" style="width:100%">       
            </div>
        </div>
        

        <!-- <h2>Comparison to Baselines </h2> -->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="images/scatter.jpg" style="width:100%">
                <p style="text-align: justify;"><b>Figure 2:</b> Comparison of QC performance between three baseline methods (UNet, ResNet-34, ResNet-50) and the proposed method (QCResUNet) for segmentations generated using nnUNet and SegGen (top row) as well as  DeepMedic (bottom row).</p>
            </div>
        </div>
        <hr>
    </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://link.springer.com/chapter/10.1007/978-3-031-43901-8_17"
                   class="list-group-item">
                    <img src="images/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{qiu2023qcresunet,
                title={QCResUNet: Joint Subject-Level and Voxel-Level 
                        Prediction of Segmentation Quality},
                author={Qiu, Peijie 
                    and Chakrabarty, Satrajit 
                    and Nguyen, Phuc 
                    and Ghosh, Soumyendu Sekhar 
                    and Sotiras, Aristeidis},
                booktitle={International Conference on Medical Image Computing 
                            and Computer-Assisted Intervention},
                pages={173--182},
                year={2023},
                organization={Springer}
              }
        </div>
    </div>

    <hr>

    <footer>
        <p>Acknowledgement: Thanks for the web template from <a href="https://www.vincentsitzmann.com/siren/"> SIREN</a></p> 
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
